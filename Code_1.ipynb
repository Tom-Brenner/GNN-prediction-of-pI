{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222144d8-82e0-4e7a-b9a5-df0a3edac4b4",
   "metadata": {
    "id": "222144d8-82e0-4e7a-b9a5-df0a3edac4b4"
   },
   "source": [
    "# Graph Neural Networks to Predict Protein Isoelectric Points\n",
    "\n",
    "### This notebook will become available on github as soon as my preprint [1] is available online, here: <a href=\"https://chemrxiv.org/engage/chemrxiv/article-details/6399e788ff4651008f2a4e2b\">TB 2022</a>. \n",
    "### This notebook presents the code used to train graph neural networks on prediction of protein isoelectric points, using experimental datasets curated and made available within the public domain by L. P. Kozlowski <a href=\"www.ipc2-isoelectric-point.org\">(see IPC 2) [2]</a>. As Spektral [3] is used in conjunction with Tensorflow and Keras, we need to import those (and several other) libraries; this notebook is Google Colabâ€“ready, i.e., I provide commands such as the next cell to enable working on Colab. The files provided must obviously be uploaded to the content (root) directory, though.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c44dedf-3eef-49d8-ba2c-d7ba2d49a030",
   "metadata": {
    "id": "6c44dedf-3eef-49d8-ba2c-d7ba2d49a030"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039d39db-ece3-4b48-b25c-81d5ba6a20df",
   "metadata": {
    "id": "039d39db-ece3-4b48-b25c-81d5ba6a20df"
   },
   "outputs": [],
   "source": [
    "#Install Spektral if working on Google Colab\n",
    "!pip install spektral\n",
    "#Let's install Bio as well. just because...\n",
    "!pip install Bio\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7e5e8-c7db-4c4a-8a8e-9a2c02143b6c",
   "metadata": {
    "id": "61b7e5e8-c7db-4c4a-8a8e-9a2c02143b6c"
   },
   "outputs": [],
   "source": [
    "#Import everything else\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "keras = tf.keras \n",
    "import os\n",
    "import math\n",
    "import scipy.sparse as spar\n",
    "import spektral as spk\n",
    "from spektral.data import Dataset, DisjointLoader, Graph, SingleLoader\n",
    "from spektral.transforms.normalize_adj import NormalizeAdj as NormAdj\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError, Huber\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c784572-933d-42f0-a742-c30acdb43348",
   "metadata": {
    "id": "2c784572-933d-42f0-a742-c30acdb43348"
   },
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ec3531-b844-4129-a92a-ef4d0358061d",
   "metadata": {
    "id": "f5ec3531-b844-4129-a92a-ef4d0358061d"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9034e5b-0246-45ef-aace-0518ae32719e",
   "metadata": {
    "id": "c9034e5b-0246-45ef-aace-0518ae32719e"
   },
   "source": [
    "#### We need to apply the Henderson-Hasselbalch equation differently for basic and acidic AAs. Cysteine,  asprtic acid, glutamic acid and tyrosine are acidic. Histidine, lysine and arginine are basic. Positions 8 and 9 in the following array refer to the C and N termini, the former of which is acidic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a80a5-2047-4f3a-a72a-a5839268a5f9",
   "metadata": {
    "id": "8c6a80a5-2047-4f3a-a72a-a5839268a5f9"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18033f17-681f-4d78-9889-b006e16bb975",
   "metadata": {
    "id": "18033f17-681f-4d78-9889-b006e16bb975",
    "tags": []
   },
   "outputs": [],
   "source": [
    "IsAcid = [True,True,True,False,False,False,True, True, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378ce91-2de0-4af3-b0ff-9291a7f9cf81",
   "metadata": {
    "id": "c378ce91-2de0-4af3-b0ff-9291a7f9cf81"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055f136-fbfe-4674-b1f7-439f53f7ba2d",
   "metadata": {
    "id": "6055f136-fbfe-4674-b1f7-439f53f7ba2d"
   },
   "source": [
    "#### Here are some pKa values to start experimenting with, taken from [4]. These will come in handy when calculating baseline isoelectric point (pI) values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584ebc02-59e1-4e08-8c8e-132b5d9f621a",
   "metadata": {
    "id": "584ebc02-59e1-4e08-8c8e-132b5d9f621a"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c7fad-1d74-408d-ae68-6e1d92c60162",
   "metadata": {
    "id": "c12c7fad-1d74-408d-ae68-6e1d92c60162"
   },
   "outputs": [],
   "source": [
    "# pKa constants taken from [4], cf. AA_List\n",
    "pKa_List = [8.3, 3.9, 4.3, 6, 10.5, 12.5, 10.1, 2.4, 9.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabab56b-3a38-4fbf-a635-730b63ed5345",
   "metadata": {
    "id": "fabab56b-3a38-4fbf-a635-730b63ed5345"
   },
   "outputs": [],
   "source": [
    "AA_List = ['C','D','E','H','K','R','Y'];\n",
    "AA_List_Full = ['A','R','N','D','C','Q','E','G','H','I','L','K','M','F','P','S','T','W','Y','V']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea627c8-bb92-4655-86c6-cfe146c9a6d9",
   "metadata": {
    "id": "9ea627c8-bb92-4655-86c6-cfe146c9a6d9"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a41fd-b590-472c-96cc-a0ea7cc7b504",
   "metadata": {
    "id": "bf4a41fd-b590-472c-96cc-a0ea7cc7b504"
   },
   "source": [
    "####  Let's create a function to prepare datasets, reading data from a file with an 'exp_pI' column, containing the experimental pI values, and a string of amino acids (AAs) for each AA sequence. \n",
    "####  The Calc_Charge function calculates the charge (in atomic units) on a protein using the number of residues of the nine AAs of residues of interest mentioned before, at single pH value, and utilizing nine pKa values used as input. Note that the IsAcid values tell the calculation whether the protonated form of the AA has a charge of 0 (True, acidic AAs) or +1 (False, basic AAs). Everything else is just a simple manipulation of the Henderson-Hasselbalch equation. We will need this function to get the baseline, zeroth-approximation pI values.   \n",
    "#### pI_HH solves numerically for the root of the Calc_Charge function. It would be extremely simple to calculate the charge for pH from 0 to 14 using 0.0001 pH unit steps; our later calculations are much heavier. Having seen attempts to leverage inefficient methods to speed up such simple numerical calculations, however, I deemed it appropriate to show how to do it properly with a quick and dirty Newton method implementation; to avoid the \"Newton Method's Spiral of Death\", the largest pH step allowed is 0.25 pH units. The root of the function is found when the Newton's method step is below 0.0025 pH units, and the charge on the protein is under 10**-20. That's a pretty small number.  \n",
    "####  The variables returned are the labels (difference between exp_pI and calculated baseline pI values), the numbered AA sequnces, a scalar telling us how many AA sequences are in the dataset (nu_samples) and an array containing the lengths of all AA sequences (Nodes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06555a53-2479-40e2-bda2-4107908a3545",
   "metadata": {
    "id": "06555a53-2479-40e2-bda2-4107908a3545"
   },
   "outputs": [],
   "source": [
    "def prep_dataset(IsAcid,pKa_List,AA_List, AA_List_Full,file_path,DirectInput=None):\n",
    "    sequences = []\n",
    "    Nodes = []\n",
    "    if DirectInput:\n",
    "        Nodes = [len(DirectInput)]\n",
    "        exp_pI = [7]\n",
    "        nu_samples = len(exp_pI)\n",
    "        Nu_AA1 = np.zeros((nu_samples, 7))\n",
    "        Nu_AA2 = np.ones((nu_samples, 2))\n",
    "        Nu_AA = np.concatenate((Nu_AA1, Nu_AA2), axis=1)\n",
    "        s = [20]*Nodes\n",
    "        for jj in range(20):\n",
    "            positions = [q.start() for q in re.finditer(AA_List_Full[jj], DirectInput)]\n",
    "            for pos in positions:\n",
    "                s[int(pos)] = int(jj)\n",
    "        sequences.append(s)\n",
    "        for jj in range(0,7):\n",
    "            Nu_AA[j,jj] = DirectInput.count(AA_List[jj])\n",
    "    elif file_path.lower().endswith('csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "        exp_pI = df.iloc[:,0]; #or df['exp_pI']\n",
    "        nu_samples = len(exp_pI)\n",
    "        Nu_AA1 = np.zeros((nu_samples, 7))\n",
    "        Nu_AA2 = np.ones((nu_samples, 2))\n",
    "        Nu_AA = np.concatenate((Nu_AA1, Nu_AA2), axis=1)\n",
    "        for j in range(0,nu_samples): \n",
    "            one_sequence = df.loc[j][1]\n",
    "            for jj in range(0,7):                \n",
    "                Nu_AA[j,jj] = one_sequence.count(AA_List[jj])\n",
    "            s = [20]*len(one_sequence)\n",
    "            for jk in range(20):\n",
    "                positions = [q.start() for q in re.finditer(AA_List_Full[jk], one_sequence)]\n",
    "                for pos in positions:\n",
    "                    s[int(pos)] = int(jk)\n",
    "            sequences.append(s)\n",
    "            Nodes.append(int(len(one_sequence)))\n",
    "    elif file_path.lower().endswith('fasta'):\n",
    "        fasta_sequences = SeqIO.parse(open(file_path),'fasta')\n",
    "        nu_samples = len([0 for fasta in fasta_sequences])\n",
    "        exp_pI = [7]*nu_samples\n",
    "        Nu_AA1 = np.zeros((nu_samples, 7))\n",
    "        Nu_AA2 = np.ones((nu_samples, 2))\n",
    "        Nu_AA = np.concatenate((Nu_AA1, Nu_AA2), axis=1)\n",
    "        fasta_sequences = SeqIO.parse(open(file_path),'fasta') #and again, 'cos it's SSSSSSSTUPID!\n",
    "        j = 0\n",
    "        for fasta in fasta_sequences:\n",
    "            one_sequence = str(fasta.seq)\n",
    "            for jj in range(0,7):\n",
    "                Nu_AA[j,jj] = one_sequence.count(AA_List[jj])\n",
    "            s = [20]*len(one_sequence)\n",
    "            for jk in range(20):\n",
    "                positions = [q.start() for q in re.finditer(AA_List_Full[jk], one_sequence)]\n",
    "                for pos in positions:\n",
    "                    s[int(pos)] = int(jk)\n",
    "            sequences.append(s)\n",
    "            Nodes.append(int(len(one_sequence)))\n",
    "            j+=1 \n",
    "                \n",
    "                    \n",
    "            \n",
    "    def Calc_Charge(pH, IsAcid, pKa_List, Nu_AA):\n",
    "        Charge = 0\n",
    "        dq_dx = 0\n",
    "        for j in range(0,len(Nu_AA)):\n",
    "            Ratio = 10**(pKa_List[j]-pH)\n",
    "            Frac = Ratio/(1+Ratio)\n",
    "            Charge = Charge + Nu_AA[j]*(-IsAcid[j] + Frac)\n",
    "            dq_dx = dq_dx - math.log(10,math.exp(1))*Nu_AA[j]*Ratio*(1+Ratio)**-2\n",
    "\n",
    "        return Charge, dq_dx\n",
    "    \n",
    "    def pI_HH(exp_pI, IsAcid, pKa_List, Nu_AA):\n",
    "        x = exp_pI + 1\n",
    "        corrx = 1 #Ensure \"correction\" onto exp_pI on the first step\n",
    "        while abs(corrx) >= .0025: \n",
    "            x = x - corrx\n",
    "            Charge, dq_dx = Calc_Charge(x, IsAcid, pKa_List, Nu_AA)\n",
    "        \n",
    "            if abs(dq_dx) > 10**-20:\n",
    "                corrx = Charge/dq_dx\n",
    "                corrx = np.sign(corrx)*min(.25,abs(corrx))\n",
    "            else:\n",
    "                corrx = 0\n",
    "            \n",
    "        return x      \n",
    "\n",
    "    \n",
    "    C = np.zeros(0)\n",
    "    for j in range(0,nu_samples):\n",
    "        c = pI_HH(exp_pI[j], IsAcid, pKa_List, Nu_AA[j])\n",
    "        C = np.append(C,[c],axis=0)\n",
    "    \n",
    "    labels = exp_pI - C\n",
    "    return sequences, Nodes, nu_samples, labels, C\n",
    "    \n",
    "\n",
    "             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V5Fm3DkfWqxD",
   "metadata": {
    "id": "V5Fm3DkfWqxD"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DhhUA380WwAs",
   "metadata": {
    "id": "DhhUA380WwAs"
   },
   "source": [
    "#### Let's prepare a combined dataset of the peptide and protein training sets. We'll save it under Combined_Training.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LtQN8xMqWgVl",
   "metadata": {
    "id": "LtQN8xMqWgVl"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/IPC2_protein_75.csv')\n",
    "df1 = pd.read_csv('/content/IPC2_peptide_75.csv')\n",
    "exp_pI = df.iloc[:,0]; #or df['exp_pI']\n",
    "exp_pI1 = df1.iloc[:,0];\n",
    "S = df.iloc[:,1]; #or df['sequence']\n",
    "S1 = df1.iloc[:,1]; \n",
    "exp_pI = np.concatenate((exp_pI,exp_pI1),axis=0)\n",
    "S = np.concatenate((S,S1),axis=0)\n",
    "De = {'exp_pI': exp_pI, 'sequence': S}\n",
    "df = pd.DataFrame(data=De)\n",
    "df.to_csv('Combined_Training.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e19318-7460-41ff-a194-7bcabc91a588",
   "metadata": {
    "id": "26e19318-7460-41ff-a194-7bcabc91a588"
   },
   "outputs": [],
   "source": [
    "sequences, Nodes, nu_samples, labels, C = prep_dataset(IsAcid,pKa_List,AA_List, AA_List_Full,'/content/Combined_Training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d0706-0215-4e49-baa2-f07ca9262ded",
   "metadata": {
    "id": "0e7d0706-0215-4e49-baa2-f07ca9262ded"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884278e1-3f29-40c3-a63b-294606e756ea",
   "metadata": {
    "id": "884278e1-3f29-40c3-a63b-294606e756ea"
   },
   "source": [
    "#### Let's load the physical descriptors. They are already normalized for our convenience; the original values are given in Ref. [1],[5]â€“[7]. For reference, these descripotors are ordered as ionization energy, electron affinity, polarizability, dipole moment, hydrophobicity, volume, and alpha-helix and beta-sheet propensity values; see original publications for more details. The first twenty entries correspond to our AA_List_Full array, and the twenty-first value is an average, reserved for unknown AAs in the sequence.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ccb107-2eb7-4ef2-a3cd-a53eece8f0a0",
   "metadata": {
    "id": "81ccb107-2eb7-4ef2-a3cd-a53eece8f0a0"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b32a50c-4137-45ed-a2b0-2a3eb1a0ff43",
   "metadata": {
    "id": "7b32a50c-4137-45ed-a2b0-2a3eb1a0ff43"
   },
   "outputs": [],
   "source": [
    "col21_28 = pd.read_csv('/content/X21to28.csv',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbe4662-ee58-49e6-8704-70d8ea34ac39",
   "metadata": {
    "id": "cfbe4662-ee58-49e6-8704-70d8ea34ac39"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6cee42-cfa4-4546-9df3-c00c70d754d0",
   "metadata": {
    "id": "6a6cee42-cfa4-4546-9df3-c00c70d754d0"
   },
   "source": [
    "#### The following builds the dataset used by Spektral. Reading the original documentation from [3] will come in handy here; note that we allow for three variations: 21 node features given as a one-hot encoding of the AAs, eight node features corresponding to the physical descriptors, and a combination of those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da68e9c-b00c-4957-b8d2-4efdd1253d33",
   "metadata": {
    "id": "3da68e9c-b00c-4957-b8d2-4efdd1253d33"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d0aa7-f7d1-4347-94b4-2c674157a517",
   "metadata": {
    "id": "f12d0aa7-f7d1-4347-94b4-2c674157a517"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Load data\n",
    "################################################################################\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, nu_samples, n_AA, **kwargs): #n_nodes, sequences, labels, ):\n",
    "        self.nu_samples = nu_samples\n",
    "        self.n_AA = n_AA\n",
    "        #self.n_nodes = n_nodes\n",
    "        #self.sequences = sequences\n",
    "        for key, value in kwargs.items():\n",
    "            self.key = value\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        def make_graph(ZZ):\n",
    "            n = self.Nodes[ZZ]\n",
    "            AA = self.sequences[ZZ]\n",
    "            exp_values = self.col21_28\n",
    "            JD = self.Just_Descriptors\n",
    "            # Node features\n",
    "            if exp_values is None:\n",
    "                x = np.zeros((n, self.n_AA))\n",
    "                x[np.arange(n), AA] = 1\n",
    "            else:\n",
    "                x = np.zeros((n, self.n_AA+8))\n",
    "                for j in range(self.n_AA):\n",
    "                    s = [i for i,Y in enumerate(AA) if Y==j]   \n",
    "                    if s:\n",
    "                        x[s, 21:] = [exp_values.iloc[:,j] for _ in range(len(s))]\n",
    "                if JD:\n",
    "                    x = np.delete(x,np.s_[:21],1)\n",
    "                \n",
    "           \n",
    "                \n",
    "                \n",
    "        \n",
    "            # Edges\n",
    "            a = np.eye(n)\n",
    "            a[np.arange(n-1),np.arange(1,n)] = 1\n",
    "            a[np.arange(1,n),np.arange(n-1)] = 1\n",
    "            a = spar.csr_matrix(a)\n",
    "\n",
    "            # Labels\n",
    "            y = self.labels[ZZ]\n",
    "\n",
    "            return Graph(x=x, a=a, y=y)\n",
    "\n",
    "        # We must return a list of Graph objects\n",
    "        return [make_graph(ZZ) for ZZ in range(self.nu_samples)] \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2524c-be10-4ff5-b3dd-dde1f1527dc2",
   "metadata": {
    "id": "19b2524c-be10-4ff5-b3dd-dde1f1527dc2"
   },
   "source": [
    "### Let's create the 29-feature dataset. I'll be showing only the (smaller) protein dataset; later, I'll show the work on the combined (protein+peptide) dataset. It is trivial to do the work on the separate peptide dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829a354-78b4-4c8b-9c4f-ae3682185498",
   "metadata": {
    "id": "7829a354-78b4-4c8b-9c4f-ae3682185498"
   },
   "outputs": [],
   "source": [
    "dataset = MyDataset(nu_samples,n_AA=21,Nodes=Nodes,sequences=sequences,labels=labels,\n",
    "                    col21_28=col21_28,Just_Descriptors=False,\n",
    "                    transforms=NormAdj())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e4b8ea-6929-4a8c-9497-faebb065e348",
   "metadata": {
    "id": "d1e4b8ea-6929-4a8c-9497-faebb065e348"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac6d24-84c6-48fc-9482-f6df7d68f7d6",
   "metadata": {
    "id": "4eac6d24-84c6-48fc-9482-f6df7d68f7d6"
   },
   "outputs": [],
   "source": [
    "#We want to always use the same training and validation sets\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa0ca4f-7a53-48b6-b910-e41486d0c321",
   "metadata": {
    "id": "7fa0ca4f-7a53-48b6-b910-e41486d0c321",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#To divide into 85% training and 15% validation; we don't touch the testing set\n",
    "#In this here town we like our batch size to be kept at 32\n",
    "idxs = np.random.permutation(len(dataset))  # Random split\n",
    "split = int(0.85 * len(dataset))\n",
    "idx_tr, idx_va = np.split(idxs, [split])\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4ec1a0-4e1e-4403-83d3-b182042263cb",
   "metadata": {
    "id": "7d4ec1a0-4e1e-4403-83d3-b182042263cb"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3228714b-ce15-4a6b-9844-238bae9fdc5e",
   "metadata": {
    "id": "3228714b-ce15-4a6b-9844-238bae9fdc5e"
   },
   "source": [
    "#### Here comes the protein test set: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a82d3-5479-493f-b0f5-7a6998dfd5e9",
   "metadata": {
    "id": "3c1a82d3-5479-493f-b0f5-7a6998dfd5e9"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a4684-d64f-44b0-b753-2c98c9d981e3",
   "metadata": {
    "id": "532a4684-d64f-44b0-b753-2c98c9d981e3"
   },
   "outputs": [],
   "source": [
    "sequencesTE, NodesTE, nu_samplesTE, labelsTE, C = prep_dataset(IsAcid,pKa_List,AA_List, AA_List_Full,'/content/IPC2_protein_25.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09faaa2a-dd57-458f-8ada-db0dd73d414a",
   "metadata": {
    "id": "09faaa2a-dd57-458f-8ada-db0dd73d414a"
   },
   "outputs": [],
   "source": [
    "datasetTE = MyDataset(nu_samplesTE,n_AA=21,Nodes=NodesTE,\n",
    "                      sequences=sequencesTE,labels=labelsTE,col21_28=col21_28,\n",
    "                      Just_Descriptors=False,transforms=NormAdj())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460a931-8abd-43e4-8f1d-19ab90e8d8d9",
   "metadata": {
    "id": "5460a931-8abd-43e4-8f1d-19ab90e8d8d9"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1ed7d1-8ad4-45ab-ab43-0d25df565c11",
   "metadata": {
    "id": "5e1ed7d1-8ad4-45ab-ab43-0d25df565c11"
   },
   "source": [
    "#### ...and here's the peptide test set. We'll load it and just keep it till we need it. As noted earlier, we won't train on the peptide dataset, but rather turn directly to the combined peptide+protein dataset... later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46f3b7-bbee-4de2-820f-14460c2bf85b",
   "metadata": {
    "id": "8b46f3b7-bbee-4de2-820f-14460c2bf85b"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5bf474-6315-4f17-a7c9-6bc8ac8a5356",
   "metadata": {
    "id": "2d5bf474-6315-4f17-a7c9-6bc8ac8a5356"
   },
   "outputs": [],
   "source": [
    "sequences_pepTE, Nodes_pepTE, nu_samples_pepTE, labels_pepTE, C = prep_dataset(IsAcid,pKa_List,AA_List, AA_List_Full,'/content/IPC2_peptide_25.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7539f-a515-4816-9637-17e942eb2872",
   "metadata": {
    "id": "bbf7539f-a515-4816-9637-17e942eb2872"
   },
   "outputs": [],
   "source": [
    "dataset_pepTE = MyDataset(nu_samples_pepTE,n_AA=21,Nodes=Nodes_pepTE,\n",
    "                      sequences=sequences_pepTE,labels=labels_pepTE,col21_28=col21_28,\n",
    "                      Just_Descriptors=False,transforms=NormAdj())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e17d7b-de9a-47a9-823d-bd7b145c6e0f",
   "metadata": {
    "id": "f6e17d7b-de9a-47a9-823d-bd7b145c6e0f"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a7969b-ab00-4c3d-9350-9f2be5005968",
   "metadata": {
    "id": "e7a7969b-ab00-4c3d-9350-9f2be5005968"
   },
   "source": [
    "####  ...Now let's separate the training dataset into the actual training and validation sets, and get the DisjointLoader on board. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0809cc68-a0ba-4e3c-8864-3536b3f68f3f",
   "metadata": {
    "id": "0809cc68-a0ba-4e3c-8864-3536b3f68f3f"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a91180-27d1-4d85-8148-bc674196ae23",
   "metadata": {
    "id": "17a91180-27d1-4d85-8148-bc674196ae23"
   },
   "outputs": [],
   "source": [
    "dataset_tr, dataset_va = dataset[idx_tr], dataset[idx_va] #training and validation\n",
    "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=1000)\n",
    "loader_va = DisjointLoader(dataset_va, batch_size=batch_size)\n",
    "loader_te = DisjointLoader(datasetTE, batch_size=batch_size) #test set\n",
    "loader_pep_te = DisjointLoader(dataset_pepTE, batch_size=batch_size) #peptide test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1931f3-d7ce-4e51-bec2-3dcfb47db055",
   "metadata": {
    "id": "2b1931f3-d7ce-4e51-bec2-3dcfb47db055"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bc3336-ae35-40a6-93cc-f3b152c38b7b",
   "metadata": {
    "id": "d7bc3336-ae35-40a6-93cc-f3b152c38b7b"
   },
   "outputs": [],
   "source": [
    "#### The GCN architecture used for this paper is given below, but for brevity, I'll only show the runs with GIN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5061b12-5b7a-4bf2-b035-e27c3319dd9b",
   "metadata": {
    "id": "e5061b12-5b7a-4bf2-b035-e27c3319dd9b"
   },
   "outputs": [],
   "source": [
    "class GCN(keras.models.Model):\n",
    "    \n",
    "    def __init__(self, channels, n_layers, dropout_rate=.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = spk.layers.GCNConv(channels)\n",
    "        self.convs = []\n",
    "        \n",
    "        for _ in range(1, n_layers):\n",
    "            self.convs.append(\n",
    "                spk.layers.GCNConv(channels)\n",
    "            )\n",
    "        self.pool = spk.layers.GlobalAvgPool()\n",
    "        self.dense1 = keras.layers.Dense(channels, activation='relu')\n",
    "        self.dropout = keras.layers.Dropout(dropout_rate)\n",
    "        self.dense2 = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, i = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        for conv in self.convs:\n",
    "            x = conv([x, a])\n",
    "        x = self.pool([x, i])\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628595a6-f991-40c5-a441-674aa9e94ba7",
   "metadata": {
    "id": "628595a6-f991-40c5-a441-674aa9e94ba7"
   },
   "outputs": [],
   "source": [
    "class GIN(keras.models.Model):\n",
    "    def __init__(self, channels, n_layers,dropout_rate=.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = spk.layers.GINConv(channels, epsilon=None, mlp_hidden=[channels, channels],\n",
    "                                       kernel_initializer=keras.initializers.glorot_uniform(seed=0))\n",
    "        self.convs = []\n",
    "        for _ in range(1, n_layers):\n",
    "            self.convs.append(spk.layers.GINConv(channels, epsilon=None, mlp_hidden=[channels, channels],\n",
    "                              kernel_initializer=keras.initializers.glorot_uniform(seed=0)))\n",
    "        self.pool = spk.layers.GlobalAvgPool()\n",
    "        self.dense1 = keras.layers.Dense(channels, activation=\"relu\",\n",
    "                                         kernel_initializer=keras.initializers.glorot_uniform(seed=11))\n",
    "        self.dropout = keras.layers.Dropout(dropout_rate)\n",
    "        self.dense2 = keras.layers.Dense(1,kernel_initializer=keras.initializers.glorot_uniform(seed=11))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, i = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        for conv in self.convs:\n",
    "            x = conv([x, a])\n",
    "        x = self.pool([x, i])\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04553008-8ea1-490d-b434-6033d45dfbfd",
   "metadata": {
    "id": "04553008-8ea1-490d-b434-6033d45dfbfd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn =  MeanSquaredError()\n",
    "optimizer = Adam(learning_rate=2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348bdd6f-d9e4-462c-915b-7d6935c25e13",
   "metadata": {
    "id": "348bdd6f-d9e4-462c-915b-7d6935c25e13"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c5390e-c3cc-431d-aedc-54cc38586222",
   "metadata": {
    "id": "87c5390e-c3cc-431d-aedc-54cc38586222"
   },
   "source": [
    "#### The following two cells contain code to run model training. Important points to note: the decoration @tf.function allows to ensure the neural network model is fed with the appropriate input shapes. Run_model checks for the validation loss, and will retrieve the best weights with respect to the validation set. The genetic algorithmâ€“like preselector is presented next (simple_GA). The ouliers calculated by the outlier_calculation function follow the definition used by [2].  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc8b88-3e8e-4bd2-9b64-36a3e0bd3b56",
   "metadata": {
    "id": "4adc8b88-3e8e-4bd2-9b64-36a3e0bd3b56"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf40772-bf5b-45d7-86ff-39d0fe4f459e",
   "metadata": {
    "id": "2cf40772-bf5b-45d7-86ff-39d0fe4f459e"
   },
   "outputs": [],
   "source": [
    "def Define_Run_and_Evaluate(load_tr, mod_name,load_va,load_te,patience_steps,loss_input):\n",
    "    @tf.function(input_signature=load_tr.tf_signature(), reduce_retracing=True)\n",
    "    def train_mod_name(inputs,target):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = mod_name(inputs, training=True)\n",
    "            loss = loss_fn(target, predictions) #+ sum(model.losses)\n",
    "        gradients = tape.gradient(loss, mod_name.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, mod_name.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    best_weights, val_losses = Run_model(loader_tr=load_tr,loader_va=load_va,model_input=mod_name,patience_input=patience_steps,\n",
    "                             loss_input=loss_input,decorated_func = train_mod_name)\n",
    "    mod_name.set_weights(best_weights)  # Load best model\n",
    "    test_loss = evaluate(loader=load_te,model_input=mod_name)\n",
    "    #print(test_loss)\n",
    "    return best_weights, val_losses, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0bfe39-e3af-48f6-8852-efe5bada09b0",
   "metadata": {
    "id": "cf0bfe39-e3af-48f6-8852-efe5bada09b0"
   },
   "outputs": [],
   "source": [
    "def evaluate(loader,model_input):\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model_input(inputs, training=False)\n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])\n",
    "\n",
    "def Run_model(loader_tr,loader_va,model_input,patience_input,loss_input,decorated_func):\n",
    "    epoch = step = 0\n",
    "    best_val_loss = loss_input\n",
    "    patience = patience_input\n",
    "    results = []\n",
    "    val_losses = []\n",
    "    for batch in loader_tr:\n",
    "        step += 1\n",
    "        #inputs, target = batch\n",
    "        loss = decorated_func(*batch)\n",
    "        results.append((loss))\n",
    "        if step == loader_tr.steps_per_epoch:\n",
    "            step = 0\n",
    "            epoch += 1\n",
    "            if epoch == 1:\n",
    "                best_weights = model_input.get_weights()\n",
    "\n",
    "            # Compute validation loss \n",
    "            val_loss = evaluate(loader_va,model_input)\n",
    "            val_losses.append(val_loss)\n",
    "            # Check if loss improved for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience = patience_input\n",
    "                best_weights = model_input.get_weights()\n",
    "            else:\n",
    "                patience -= 1\n",
    "                if patience == 0:\n",
    "                    break\n",
    "            results = []\n",
    "\n",
    "    return best_weights, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9898d81b-f231-416a-9bbb-0e60acb3705f",
   "metadata": {
    "id": "9898d81b-f231-416a-9bbb-0e60acb3705f"
   },
   "outputs": [],
   "source": [
    "def simple_GA(opt_weights, y, weights):\n",
    "    z = len(np.asarray(opt_weights,dtype=object))\n",
    "    for k in np.arange(z):  \n",
    "        master_matrix = np.asarray(weights[y[0]][k])\n",
    "        master_matrix = np.stack((master_matrix,np.asarray(weights[y[1]][k])),axis=0)\n",
    "        master_matrix1 = np.asarray(weights[y[2]][k])\n",
    "        master_matrix1 = np.stack((master_matrix1,np.asarray(weights[y[3]][k])),axis=0)\n",
    "        master_matrix = np.concatenate((master_matrix,master_matrix1),axis=0)\n",
    "\n",
    "        kth_weights = np.ndarray.flatten(best_weights[k])\n",
    "        rand_inds = np.ndarray.flatten(np.random.randint(0, 4, size=kth_weights.shape))\n",
    "        for j in np.arange(len(rand_inds)):\n",
    "            kth_weights[j] = np.ndarray.flatten(master_matrix[rand_inds[j]])[j]\n",
    "        opt_weights[k] = kth_weights.reshape(master_matrix[0].shape)\n",
    "    \n",
    "    out = opt_weights\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e1ad0-74bb-40be-8017-1fdd70470119",
   "metadata": {
    "id": "8e5e1ad0-74bb-40be-8017-1fdd70470119"
   },
   "outputs": [],
   "source": [
    "def outlier_calculation(model_name,loader):\n",
    "    output = 0\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model_name(inputs, training=False)\n",
    "        output += sum((abs(np.array(target-pred))>.5))\n",
    "        if step == loader.steps_per_epoch:\n",
    "            print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b590b11-88aa-4045-baed-d7ec108355bd",
   "metadata": {
    "id": "8b590b11-88aa-4045-baed-d7ec108355bd"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b127d-399a-4f6c-b79a-5d073b13369f",
   "metadata": {
    "id": "628b127d-399a-4f6c-b79a-5d073b13369f"
   },
   "source": [
    "#### Let's see how well our combined dataset is learned by a four-layer GIN model..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe79ec1-161c-486c-b7e3-c53e1bd97b29",
   "metadata": {
    "id": "cfe79ec1-161c-486c-b7e3-c53e1bd97b29"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b9757-b468-4b35-a8e1-210a5ae24238",
   "metadata": {
    "id": "822b9757-b468-4b35-a8e1-210a5ae24238"
   },
   "outputs": [],
   "source": [
    "for _ in np.arange(1): #Let's let a single run suffice \n",
    "    weights = []\n",
    "    vals = []\n",
    "    for _ in np.arange(36):\n",
    "        model_pro = GIN(channels=8,dropout_rate=.15,n_layers=4)\n",
    "        best_weights, val_losses, test_loss = Define_Run_and_Evaluate(loader_tr, model_pro,loader_va,\n",
    "                                                                      loader_te,\n",
    "                                                                      patience_steps=1,loss_input=0)\n",
    "        weights.append(best_weights)\n",
    "        vals.append(val_losses[-1])\n",
    "\n",
    "    vals = np.asarray(vals)\n",
    "    y=np.argsort(np.ndarray.flatten(vals))\n",
    "    opt_weights = simple_GA(best_weights, y, weights)\n",
    "\n",
    "    z = len(np.asarray(opt_weights,dtype=object))\n",
    "    model_pro.set_weights(opt_weights) \n",
    "    best_weights, val_losses, test_loss = Define_Run_and_Evaluate(loader_tr, model_pro,loader_va,\n",
    "                                                                  loader_pep_te,\n",
    "                                                                  patience_steps=40,loss_input=np.Inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93630e33-a0b3-4a4e-97e4-1c44e345af68",
   "metadata": {
    "id": "93630e33-a0b3-4a4e-97e4-1c44e345af68"
   },
   "outputs": [],
   "source": [
    "test_loss = evaluate(loader=loader_te,model_input=model_pro)\n",
    "print(test_loss**.5)\n",
    "test_loss = evaluate(loader=loader_pep_te,model_input=model_pro)\n",
    "print(test_loss**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rGqGoTKky8_h",
   "metadata": {
    "id": "rGqGoTKky8_h"
   },
   "outputs": [],
   "source": [
    "model_pro.save_weights('newer_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pWYZYAhvHjQd",
   "metadata": {
    "id": "pWYZYAhvHjQd"
   },
   "outputs": [],
   "source": [
    "outlier_calculation(model_name=model_pro,loader=loader_te)\n",
    "outlier_calculation(model_name=model_pro,loader=loader_pep_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a1ca7-bc95-431d-8066-150b77aca4fb",
   "metadata": {
    "id": "427a1ca7-bc95-431d-8066-150b77aca4fb"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ec1b3-68e1-431f-8f27-bdce704944a3",
   "metadata": {
    "id": "949ec1b3-68e1-431f-8f27-bdce704944a3"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be55374-3d33-45f3-bf6f-7a0135640ee6",
   "metadata": {
    "id": "1be55374-3d33-45f3-bf6f-7a0135640ee6"
   },
   "source": [
    "### If everything goes to plan, you should get RMSE values of about 0.87â€“0.88 and 0.27â€“0.28 on the protein and peptide test sets. Now let's predict a pI for a new AA sequence. Feel free to use my 'original_weights'. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c25fd-2a06-4a8c-88e8-15b79052d260",
   "metadata": {
    "id": "b79c25fd-2a06-4a8c-88e8-15b79052d260"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7361a42f-64a0-4ad6-aa85-5c1a4212d27b",
   "metadata": {
    "id": "7361a42f-64a0-4ad6-aa85-5c1a4212d27b"
   },
   "outputs": [],
   "source": [
    "Your_Pro = [''] #copy and paste AA sequence; please keep it to a single string\n",
    "#alternatively, read in a FASTA file below\n",
    "Seq, Nod, Nus, Labs, computed_pI = prep_dataset(IsAcid,pKa_List,AA_List, AA_List_Full,'/content/filename.fasta')\n",
    "your_dataset = MyDataset(Nus,n_AA=21,Nodes=Nod,sequences=Seq,labels=Labs,\n",
    "                    col21_28=col21_28,Just_Descriptors=False,\n",
    "                    transforms=NormAdj())\n",
    "your_loader = DisjointLoader(your_dataset)\n",
    "your_model = GIN(channels=8,dropout_rate=.15,n_layers=4)\n",
    "your_model.load_weights('original_weights')\n",
    "_, B = evaluate(your_loader,your_model)\n",
    "MM = computed_pI + B\n",
    "print(f'The predicted pI is {np.squeeze(MM)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6235db-8e39-41cd-80e5-cfcdefb70084",
   "metadata": {
    "id": "stFE58vt5MBl"
   },
   "source": [
    "### References\n",
    "\n",
    "#### 1. <a href = \"https://chemrxiv.org/engage/chemrxiv/article-details/6399e788ff4651008f2a4e2b\"> Brenner 2022 </a>\n",
    "#### 2. <a href = http://ipc2.mimuw.edu.pl/>Kozlowski 2021 </a>\n",
    "#### 3. <a href = https://github.com/danielegrattarola/spektral/> Spektral </a>\n",
    "#### 4. <a href = https://www.wiley.com/en-ie/Solomons%27+Organic+Chemistry,+12th+Edition,+Global+Edition-p-9781119248972> Solomons, Fryhle, & Snyder 2017 </a>\n",
    "#### 5. <a href = https://journals.aps.org/pre/abstract/10.1103/PhysRevE.75.011920>Moret & Zebende 2007 </a>\n",
    "#### 6. <a href = https://pubmed.ncbi.nlm.nih.gov/3209351/>Fauchere, Charton, Kier,Â Verloop & Pliska 1988 </a>\n",
    "#### 7. <a href = https://www.pnas.org/doi/10.1073/pnas.96.22.12524> Koehl & Levitt, 1999 </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae953537-8b94-4963-82e9-b0750e2ba0f8",
   "metadata": {
    "id": "ae953537-8b94-4963-82e9-b0750e2ba0f8"
   },
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
